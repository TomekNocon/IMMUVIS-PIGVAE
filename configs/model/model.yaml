_target_: "src.models.pigvae_auto_module.PLGraphAE"

graph_ae:
  _target_: "src.models.components.modules.GraphAE"
  hparams:
    input_size: 512 # 128
    num_heads: 4
    num_layers: 4
    emb_dim: 2048  # Increased from 1024 for higher capacity with 512D features
    vae: true
    dropout: 0.1
    encoder:
      graph_encoder_hidden_dim: ${model.graph_ae.hparams.input_size}
      num_node_features: 1
      num_edge_features: 0
      graph_encoder_num_heads: ${model.graph_ae.hparams.num_heads}
      graph_encoder_ppf_hidden_dim: ${multiply:${model.graph_ae.hparams.input_size},4}
      graph_encoder_num_layers: ${model.graph_ae.hparams.num_layers}
      emb_dim: ${model.graph_ae.hparams.emb_dim}
      dropout: ${model.graph_ae.hparams.dropout}
      grid_size: 6
      project: True

    decoder:
      graph_decoder_hidden_dim: ${model.graph_ae.hparams.input_size}
      graph_decoder_pos_emb_dim: ${model.graph_ae.hparams.input_size}
      graph_decoder_num_heads: ${model.graph_ae.hparams.num_heads}
      graph_decoder_ppf_hidden_dim: ${multiply:${model.graph_ae.hparams.input_size},4}
      graph_decoder_num_layers: ${model.graph_ae.hparams.num_layers}
      dropout: ${model.graph_ae.hparams.dropout}
      head_dim: ${divide:${model.graph_ae.hparams.input_size},${model.graph_ae.hparams.num_heads}}
      num_embeddings: 170 # not use if is_rope is True
      num_node_features: 1
      num_edge_features: 0
      project: True

    bottle_neck_encoder:
      graph_encoder_hidden_dim: ${model.graph_ae.hparams.input_size}
      emb_dim: ${model.graph_ae.hparams.emb_dim}
      vae: ${model.graph_ae.hparams.vae}
      activation: silu
      num_permutations: ${data.hparams.num_aug_per_sample}

    bottle_neck_decoder:
      emb_dim: ${model.graph_ae.hparams.emb_dim}
      graph_decoder_hidden_dim: ${model.graph_ae.hparams.input_size}

    property_predictor:
      emb_dim: ${model.graph_ae.hparams.emb_dim}
      property_predictor_hidden_dim: ${model.graph_ae.hparams.input_size}
      num_properties: 1

    permuter:
      graph_decoder_hidden_dim: ${model.graph_ae.hparams.input_size}
      graph_decoder_num_heads: ${model.graph_ae.hparams.num_heads}
      graph_decoder_ppf_hidden_dim: ${multiply:${model.graph_ae.hparams.input_size},4}
      dropout: ${model.graph_ae.hparams.dropout}
      num_permutations: ${data.hparams.num_aug_per_sample}
      grid_size: 6
      break_symmetry_scale: 0.05
      emb_dim: ${model.graph_ae.hparams.emb_dim}
      turn_off: False
      use_ce: False
      use_context: False
      head_dim: ${divide:${model.graph_ae.hparams.input_size},${model.graph_ae.hparams.num_heads}}
      n_components: 16  # Increased from 2 to preserve more spatial information

temperature_scheduler:
  _target_: "src.models.components.schedulers.TemperatureScheduler"
  hparams:
    initial_tau: 0.75
    final_tau: 0.15
    num_epochs: ${trainer.max_epochs}
    start_epoch: ${trainer.min_epochs}

entropy_weight_scheduler:
  _target_: "src.models.components.schedulers.EntropyWeightScheduler"
  hparams:
    initial_weight: 0.3
    final_weight: 0.03
    mode: exponential
    num_epochs: ${trainer.max_epochs}
    start_epoch: ${trainer.min_epochs}

critic:
  _target_: "src.models.components.model.Critic"
  hparams:
    kld_loss_scale: 0.0001  # Reduced to preserve more detail (was 0.01)
    kld_free_bits: 8.0  # Increased for 512D feature space - prevents posterior collapse
    perm_loss_scale: 0.8
    contrastive_loss_scale: 0.01
    temperature: 0.07
    num_aug_per_sample: ${data.hparams.num_aug_per_sample}
    vae: ${model.graph_ae.hparams.vae}
    # Detail preservation
    use_gradient_loss: true  # Enable gradient loss to preserve high-frequency details
    gradient_loss_weight: 0.5  # Weight for gradient loss (0.1-1.0 typical)

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  weight_decay: 0.00001

scheduler:
  batch_size: ${data.hparams.batch_size}
  type: cosine # one_cycle
  warmup: 0.1
  max_lr: 0.001
  div_factor: 25
  final_div_factor: 1000

compile: false
